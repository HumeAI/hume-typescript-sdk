/**
 * This file was auto-generated by Fern from our API Definition.
 */

import * as Hume from "../../../index";

/**
 * When provided, the output is a user message.
 */
export interface UserMessage {
    /** Used to manage conversational state, correlate frontend and backend data, and persist conversations across EVI sessions. */
    customSessionId?: string;
    /** Indicates if this message was inserted into the conversation as text from a [User Input](/reference/empathic-voice-interface-evi/chat/chat#send.UserInput.text) message. */
    fromText: boolean;
    /**
     * Indicates whether this `UserMessage` contains an interim (unfinalized) transcript.
     *
     * - `true`: the transcript is provisional; words may be repeated or refined in subsequent `UserMessage` responses as additional audio is processed.
     * - `false`: the transcript is final and complete.
     *
     * Interim transcripts are only sent when the [`verbose_transcription`](/reference/empathic-voice-interface-evi/chat/chat#request.query.verbose_transcription) query parameter is set to `true` in the initial handshake.
     */
    interim: boolean;
    /** Transcript of the message. */
    message: Hume.empathicVoice.ChatMessage;
    /** Inference model results. */
    models: Hume.empathicVoice.Inference;
    /** Start and End time of user message. */
    time: Hume.empathicVoice.MillisecondInterval;
    /**
     * The type of message sent through the socket; for a User Message, this must be `user_message`.
     *
     * This message contains both a transcript of the userâ€™s input and the expression measurement predictions if the input was sent as an [Audio Input message](/reference/empathic-voice-interface-evi/chat/chat#send.AudioInput.type). Expression measurement predictions are not provided for a [User Input message](/reference/empathic-voice-interface-evi/chat/chat#send.UserInput.type), as the prosody model relies on audio input and cannot process text alone.
     */
    type: "user_message";
}
